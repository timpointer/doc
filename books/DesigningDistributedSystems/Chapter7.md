# 事务的概念

## ACID的含义
A 原子性，几个操作就想一个操作一样，要么一起成功要么一起失败。
C 一致性，确保数据的一致性。
I　隔离性，并发的事务相互之间不会影响，比如不会看到其他事务执行了一半的数据。　
D　持久性，事务结束后，所有结果都被持久化下来。

## 单个对象和多对象操作

有些数据库提供操作单个对象的原子性操作，比如read-modify-write或comapre-and-set。
这种单个对象的只能算作“轻量级事务”。一个事务通常被认为是将操作多个对象的多个操作打包会一个整体来执行。

### 针对多对象事务的需求
许多数据库因为夸分片的事务实现起来太难，已经摒弃事务了。然而并没有什么本质问题阻碍在一个分布式数据库中实现事务。
但我们真的需要多对象事务吗？可以应用都只都用单对象操作吗？
有许多场景需要多对象之间协调：
- 在关系型数据库中，一条记录总是外键关联到另一张表的记录。事务能确保正确更新多张表的记录。
- 在文档性数据库中，需要属性被放到了同一个文档中，许多操作是但对象操作。但由于缺乏join功能，数据被denormailzed，更新数据时，其他视图数据也一起被同步更新，这时就要用到多对象事务。
- 在数据库中，二级索引，也被看做是第二个文档。插入数据时，要同步更新二级索引。

许多应用可以不借助事务来实现。然而没有事务导致的并发问题使得错误处理机制变的异常复杂。

## 处理错误和中断
事务的一个关键特性就是可以安全重试。虽然重试一个中断的事务是一种简单有效的错误处理机制，但它不是完美的。
- 如果事务成功，回传Ack时，网络错误导致丢失，会导致客户端错误重试。
- 如果由于负载过高导致的失败，重试只会加重服务器负担。
- 重试只对暂时性错误有效，永久性错误重试于事无补。
- 如果事务之外还有其他边际效应，比如发送email。需要其他机制来确保一致性。
- 如果重试时客户端宕机，请求数据将丢失。

# 弱隔离等级
如果两个事务操作不同的数据，他们就可以安全的并行，因为他们互相不依赖，并发问题只有发生在多个事务操作相同的数据时。
并发问题很难测试和避免，所以数据库尝试提供事务隔离。理论上隔离让你的生活更简单。但serializable级别的隔离有性能损耗，许多数据库不愿意支付价格。
因此采用更弱的隔离等级。使用弱隔离等级，并不能避免所有问题，所以我们要理解这些并发问题，知道不同隔离等级分别解决了什么问题。那样才能构建出可靠正确的应用。


## Read Committed
两个保证
1. 当读数据时，只能读到已经被提交的数据
2. 当写数据时，只能覆写已经被提交的数据

### No dirty reads
避免的问题
- 如果一个事务修改几个对象，脏读会读到数据的中间状态。
- 如果事务被中断，脏读会读到没有提交成功的数据。

### No dirty writes
- 脏写，会覆盖其他事务的中间状态。

有一种情况Read committed避免不了，就是多线程更新counter。及时避免脏读也会因为另一个原因“Preventing Lost Updates”失败。

### 实现Read Committed
通常用row级别锁来避免脏写。
要如何避免脏读，一个选项是用同一把锁。但这样会有性能问题。因为一个长时间的写事务会堵塞其他读事务。
大多数数据库采用的是，维护多个值，其他事务拿到的旧值，新的值被写事务更新，但直到最后提交，新的值才能被看到。

## Snapshot Isolation and Repeatable Read
当事务开启时，数据快照下来，事务内的查询只能看到快照中的数据。
以下场景需要这种隔离性
- 数据库备份，需要需要备份数据库当前状态的快照。
- 数据分析和完整性检查。

### 实现快照隔离
像read committed isolation一样，快照隔离也医用锁来避免脏读。然而读的时候不需要任何锁。从性能的角度看，读不堵塞任何写，写不堵塞任何读。这让数据库能在不用锁的情况下支持长读操作。
为了实现快照隔离，数据库采用一种技术称之为多版本并发控制（MVCC）。
如果数据库仅仅需要支持read commited隔离，那在事务期间只要保存对象的两个版本，一个旧版本，一个新版本的数据。但MVCC需要保持多个版本。
每个事务开启后，会产生一个唯一自增的事务ID(txid)。当一个事务往数据里面写时，被写的数据都会被打上一个（createBy=txid)的标签,被删除时打上一个（deleteBy=txid)的标签。事务期间只会读到快照时的数据。

### 可见性规则
1. 在事务开始时，数据库拿到此时正在执行的事务列表。任何由这些事务写入的数据都被忽略。
2. 任何被中断的事务被忽略
3. 任何在事务ID以后的数据被忽略
4. 其他写入对查询可见

### Indexes and snaphost isolation
1. 修改已有数据方案，索引指定到多个版本数据，当GC把旧版本被删除后，也一并删除索引中的引用。
2. 不修改已有数据方案，采用append-only B-trees,每个新版本数据都创建一个新的root节点。需要异步压缩和GC。

### Repeatable read and naming confusion
命名混淆，有些数据库中把快照隔离成为可重复读，行业并没有统一标准，所以没有知道重复读到底意味着什么。

## Preventing Lost Updates
Read commited和快照隔离主要保证了在并发写时，只读事务会遇到的一些问题。我们忽略了并发写事务中的一些问题，只讨论了脏写。
有几个其他有趣的冲突会在并发写时发生，其中最为人所知的是"lost update"，发生在read-modify-write循环中。

### 原子写操作
一些数据库支持read-modify-write原子操作，就能解决lost update问题，但不是所有场景都能转化为原子写。

### Explicit locking
如果不能用原子操作，可以明确加锁，把关键区锁起来。

### 自动发现lost updates
原子操作和锁机制都是强制read-modify-write循环顺序操作。另一个替代方案是允许并行执行，如果事务管理器检测到一个lost update，将中断事务，并强制重试。

有一些数据库支持这种功能，有些则不。
自动发现是一个优秀的特性，因为它不需要应用层使用特殊的数据库特性，比如锁和原子操作，而引入bug。

### Compare-and-set
在不提供事务的数据库中，可能可以找到compare-and-set这个原子操作。对比原来的数据，如果原来的数据没有变，则更新。但对比不能读取来自旧快照的数据，这样也避免不了lost update。

### 冲突解决和副本
在副本数据库中，避免lost update需要另一个维度：　数据在多个节点有备份，数据可能并发写入多个节点，需要额外的步骤来解决lost update。

有多主和无主数据库的场景，允许并发写入，异步同步副本。原子操作和锁在这种场景中无法应用。
然后，检测并发写是一种常用的方案，能创建针对一个值的多个冲突的副本，并在事后，采用应用代码或特殊数据结构来解决和合并这些版本。

原子操作在副本场景中能起作用，只要操作之间是顺序无关的。比如往一个set里面增加元素这种顺序无关的操作，并发写入后，可以将多个值合并起来。

另一方面，last write wins(LWW) 的冲突解决方案则在很多场景中是有问题的，但大多数数据库都才用这种默认方案。

## Write Skew and Phantoms
前面提到了drity writes和lost update，这种两种发生在写同一个对象时。
然后还有其他并发写有潜在竞争的场景。
比如：有个医院必须有两个医生在岗，现在连个医生都不舒服，需要请求，他们先查看当前在岗医生数量，然后请求。如果他们同时这么做，他们会查到有两个医生在岗，然后同时请假，最后变成没有一个医生在岗。

### 写偏差特征
可以认为写偏差是lost update的通用形式。写偏差发生在两个事务读相同的对象，写不同的对象。特数场景是读相同的对象，写相同的对象，这时就变成了lost update。
我们已经看过不同的方法来避免lost update。但对于写偏差，我们的选择更加受限：
- 原子操作不能解决这种问题，因为涉及多个对象。
- 自动检测lost update也帮不了忙。
- 如果实现不了串行隔离等级，第二选项是显示锁住涉及到的记录。

### 更多写偏差的例子
写偏差看似不常见，但一旦你意识到它，会发现很多场景有会发生这种问题：
- 订阅会议室
- 多人游戏
- 声明一个用户名，当一个网站没有用户都有一个唯一的用户名，两个用户同时创建账号。你可能使用事务来检查用户是否已经被占用，如果没有，就创建一个该用户名的账号。
然而，就想之前的例子，在快照隔离级别这并不安全。幸运的是，在这里唯一限制是一种简单的方案（第二个事务尝试创建该用户名会因为违反限制而被中断）
- 避免两次支付

### Phantoms causing write skew
幽灵导致写偏差，如果例子都有一个模式：
1. 第一个查询检查需求是否被某个查询条件所满足
2. 依赖第一个查询结果，应用做出某个决定
3. 执行这个决定，又反过来影响第一个查询结果

这个过程可以按照不同的顺序发生。比如，先写入，然后查询，最后根据查询结果决定是中断还是提交写入。

有些时候检查的是具体的记录，然而另一些时候，检查在符合条件的一个虚拟的范围，可能当前结果中并没有记录。
当一个事务中的写入操作改变另一个查询结果时，称为幽灵（phantom）。快照只能在read-only查询中避免幽灵，但在read-write事务中，幽灵会导致写偏差。

### Materializing conficts
如果幽灵问题是我们没有办法给不存在的对象上锁，或许我们可以手工在数据库中引入一个锁对象？
举例，在定会议室的例子中。我们可以创建一个时间间隔和房间的表。表中每个条记录带便一个房间的特定15分钟。你为接下来6个月所有可能的时间段都创建好对应的记录。

现在事务就可以根据这张表来锁定对应的时间段和房间。
这种方法叫做实例化冲突，因为它把幽灵转变为一个在数据库中实实在在的记录上的锁冲突。不幸的是，这种方法容易出错，并将并发控制机制泄露到应用模型中。因此，这种方法只能当做最后方案。线性隔离等级在大多数情况下是一个比较好的选择。

# Serializability
一些竞争条件能被read commited和snaphostg隔离级别避免，其他的不能。我们遇到一些写偏差和幽灵。情况很槽糕：
- 隔离级别很难理解，每个数据实现不一致
- 看应用代码，很难推测在某个隔离级别是否安全。特别是大型应用，你不知道哪些事可能并发。
- 没有好工具帮助我们发现竞争条件。理论上静态分析可以帮忙，但还不清楚如何在实际环境中使用。并发测试很难，因为不确定性。


// TODO
既然顺序隔离比其他弱隔离好这么多，那为什么不是每个人都用呢？为了回答这个问题，我们需要在看一下顺序隔离的实现和性能。大多数数据库采用三种技术来实现顺序隔离。
- 真顺序执行
- Two-phase locking两阶段锁
- 优化并发技术，比如顺序快照隔离
目前我们只讨论单节点的情况。

### Actual Serial Execution
最简单的方法避免并发问题就是完全排除并发：在一个线程中按顺序一次执行一个事务。这样我们完全排除在事务中检测和避免冲突，这被定义为顺序隔离。

尽管这是一个显而易见的想法，但数据库设计者到最近才意识到这样做是可行的。之前30年都认为多线程并发是必须的。是什么导致了转变？
两个发展导致了从新思考
- 内存变的足够便宜，所有需要执行的数据都可以放到内存中，这样事务就可以比从磁盘拿数据快许多。
- 意识到OLTP事务通常都很短，只有少量的读写操作。相反，长时间执行的分析请求，可以跑在快照隔离级别。

这种方法才被redis采用。一个单线程可能比一个多线程系统更快，但吞度量受限于单个CPU。




