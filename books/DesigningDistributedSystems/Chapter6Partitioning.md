
# Partitions(sharding)
Partitioning不同系统里面叫法不同，比如shard,region,table,vnode,vBucket，这里统一叫分片。

分片定义了每条数据属于哪个一个分片。有许多不同的分片方式。每个分片就是一个自己的小数据库，虽然数据库可能提供了跨分片的操作。
分片主要目的为了扩展（scalability）。不同的分片分布在不同节点上在一个shared-nothing的集群中。大的数据集合能被放置于不同的磁盘上，请求能被分发到不同的处理器上。

请求能被并行在多个节点上执行。

事务型和分析型数据库在分片上本质是一样的，只是系统调教不同。

我们先看下不同的分片方式和观察数据索引是怎么和分片交互的。然后讨论再平衡，有必要时需要增加和删除节点。最后看下如何路由请求和执行查询。

# Partitioning and Replication
分片和副本是两个独立的概念，可以分开使用也能组合使用。
组合使用的一般场景是，数据集有三个分片，每个分片有三个备份，一主两从。九个分片备份被分布在三个节点上，每个节点上既有主的分片，也有从的分片。

# Partitioning of Key-Value Data
假如你有一个大数据集要分片，如何决定哪条记录在哪个节点上呢？

我们的目标是把数据和请求均匀的分散到不同节点上。理论上１０个节点能处理10倍的数据和10倍的请求。
如果分片不公平，有些分片的数据和请求比其他一些大，称之为"skewed"。当发生偏差时，分片就不够有效了。当请求都集中到一个分片上时被称为"hot spot"

最简单避免热点的方式是随机分片。但这样有一个缺点，就是查询具体一条数据时，需要请求所有节点。
我们能做的更好。假设有一个简单的key-value模型，总是从主键查记录。可以参照百科全书，按照标题排序，你就能快速找到记录。

## 通过key范围分片
一种方法是为一个分片指定一个连续范围的key。如果你知道哪个key属于哪个分片，又知道哪个分片属于哪个节点。就能直接请求具体的节点。

key的范围并不需要平均分布，因为你的数据也不是平均分布的。拿百科全书举例，可能卷1包含A,B开头，卷12包含T,U,V,X,Y和Z。要适应数据本身的特性。
分片可以由管理员手动选择，也能自动选择。

在每个分片中，保持key的排序，好处是方便范围查询。能把key当做组合索引的一部分。
举例，考虑一个感应器时序型应用，拿时间戳作为key来分片，就能很容易查询到一个特定月份的数据。
然而，特定的请求会导致热点。如果按照时间戳分片，一天一个分片，那当天的数据都会写到统一分片中。而其他分片都闲着。

为了避免这个问题，你需要使用时间戳以外的其他元素作为key。比如从感应器的名字加上时间戳来做分片。假设你有许多感应器产生数据，这些请求会被均匀分散。
现在当你需要查询多个感应器的一个时间范围数据时，需要为每隔感应器单独发送请求你。

## HashKey分片
因为偏差和热点的风险，需要分布式数据库采用hash函数来为每个key决定分片。
一个好的hash函数能均分分布，即使两个string很相似，也能在数字中均匀分布。

仅仅为了分片，hash函数不需要强加密，一些数据库使用Fowler-Noll-Vo function。需要语言有自带的hash函数，但它们不都适用于分片，因为java.hashCode()，在不同进程中会得到不同的hash值。

有了合适的hash函数，你可以为每个分片指定一个范围的hash值。这种分片的好处就是key能均匀分布。
分片范围可以均匀分布也可以假均分（有时被称为constistent hashing）。
使用hash避免了需要中央控制或分布式共识。
不幸的时，采用hash分片我们丢失一个重要range分片的特性，支持范围查询。临近的key被分散在所有分片上，所以顺序被丢失了。
比如在mongodb中，如何采用hash分片。任何范围查询都需要发送到所有分片上。

### cassandra的组合分片模式
Cassandra现实了一种权衡，结合了两种策略。一个表能被声明一个联合主键，包含多个字段。只有第一个字段决定分片位置，其他字段为了数据排序用做联合索引。一个请求不能对第一个字段进行范围查询，但指定第一个字段，就能对其他字段进行范围查询。
联合索引很好处理了一对多的模型。比如一个社交网站，用户有很多post，把（user_id, update_timestamp）作为索引，就能高效的查询一个用户的一个时间段的所有post。

## 偏差负载和缓解热点
根据以上讨论，对key进行hash来决定分片能帮助减少热点情况。然而它并不能完全避免，极端情况下所有请求会打到一个key上，最终所有请求都落到一个分片上。

这种请款不常见，但也不罕见。举例，一个社交网站，一些有上百万粉丝的知名大V,当他们做一些事时就可能造成流量风暴。（所有用户都对一个用户进行评论）,hash帮不了，因为同一个key在hash还是一样的值。
当今大多数数据库不能自动解决这个问题。需要用户自己解决，比如可以在key后面增加两个随机数。那样就会有100个不同的key,这些key被分布到不同的分片上。

然后把写分布到不同的key，读的时候就需要额外工作，需要从100key中读取并组合。这需要额外的记事本（bookkeeping），只给少量的热点key添加随机数，其他大量的低请求的key没必要这么做。另外你需要记录哪些key被细分了。
将来可能数据库会解决这个问题，但当下你需要根据情况自己权衡。

# 分片与二级索引
讨论到现在都是基于key-value的数据模型，如果记录都通过主key来访问，我们能通过key决定对应分片，然后把请求路由到负责该记录的分片上。
当情况变复杂时就需要引入二级索引。在关系数据库中二级索引必不可少，也文档数据库里也很常见。许多key-value数据库为避免复杂度而不支持二级索引。而它是搜索引擎的核心。

问题在于二级索引不能简单的对应到分片上，有两种主流方式对二级索引进行分片：document-based分片和term-based分片

## 基于文档分片二级索引
基于文档的索引，也被成为local索引（相对于global）。每个分片中索引只管理自己分片的数据。
写入时，更新数据和更新索引在同一个分片里。但查询时需要采用scatter/gather的方式，请求发送给所有分片，这种操作比较耗时，延迟由最慢的一个分片决定。Mongodb采用这种方式。

## 基于Term分片二级索引
基于Term的索引，可以把索引本身看做是一个数据表，因为要分散负载，所以对索引进行分片，分片的方式和普通数据表一样，可以是范围的也可以是hash。
因为每个key的数据被放在了一起，所以查询时，只需要到特定的分片去查询即可。
缺点是写入时，因为更新数据和索引不在同一个分片上，会既慢有复杂。
理想世界中，索引应该被立即更新。而然这需要分布式事务的支持，不是所有数据库都支持。
实践中，一般都是通过异步来更新global二级索引。DynamoDB保证在秒级别同步二级索引，但也有可能更长。

# 分片再平衡
随着时间流失，一些改变在数据库中发生。
- 请求量增长，需要更多CPU来支持
- 数据集变大，需要更多磁盘和内存来存储
- 机器宕机，需要其他机器接替宕机机器的职责

所有这些改变都需要把数据和请求从一个节点移动到另一个节点。这个过程叫再平衡。
无论哪种分片方式被使用，再平衡都需要满足最小的需求：
- 再平衡以后，负载（数据，读，写请求）被均分分布在集群中
- 当再平衡发生时，服务继续可用
- 只移动必须移动的数据，最小化网络和磁盘IO

## 再平衡策略
有些不同的方式指定分片到节点。

### How not to do it: hash mod N
我们可以mod来决定数据分片，一个分片对应一个节点。假如根据10来取mod。就会有0~9个分片。当增加一个分片，就需要根据11来取mod,这样原来一个key的分片就变了，数据需要移动到其他节点上。再增加一个分片，数据又要被移动。
我们应该想一种方式来避免数据移动。

### 固定数量分片
幸运的是，有一种简单方法。创建比节点多的分片，给每个节点分配多个分片。比如一个集群10个节点，1000个分片均匀分布。
如果新加一个节点，只需要从原来节点上偷走一些分片给到它就行，删除节点，反向操作即可。
仅仅是整个分片被移动，分片数量不变，key和分片对应关系也不变。这种变更不是立即发生的，需要花时间同步大量数据，此时老分片还能继续提供服务。
理论上，你可以给节点分片不同权重的负载（分片数量）。
这种配置方式，需要数据库提前配决定分片数量，之后无法变更。虽然理论上可以切分分片和聚合分片，但大多数数据库拒绝这么做。
分片数量至少要大于节点数量。
如果数据集高度变化，要决定合适的分片数量就很难。最好是"just right"。太大再平衡和节点恢复会很昂贵，太小又有管理成本。

### 动态分片
对数据库来说，固定分片会非常不方便：如果你把分片边界搞错了，会导致所有数据到一个分片里面去，重新配置分片边界有异常痛苦。
基于这个原因，有些数据库支持动态分片。当分片数据超过配置上限，就会切分成两个分片，如果分片数量太少，又会收缩合并成一个分片。
每个分片对应一个节点，一个节点可以有多个分片。当分片分裂后，其中一个分片会被移动到其他节点上。在HBase上，这是有底层HDFS来控制的。
动态分片的好处就是分片可以根据数据集大小动态调整。
为了避免空数据冷启动（只有一个分片，无法利用多节点），可以预分片。
Mongodb的两种分片模式，key范围和hash都支持动态分片。


### 分片与节点成比例
动态分片，分片数量和数据集成比例。固定分片，每个分片的大小与数据集成比例。这两种情况，分片数量独立于节点数量。

第三种方案，被Cassandra采用。分片数量和节点成比例，每个节点有固定数量的分片。数据增长不会引起分片变化或移动，只有当新节点假如时，会从老节点中随机，选取分片进行切分，切分后的分片，有部分会被迁移到新的节点上，导致集群整体单个分片的数据量下降。

## 运维：自动或手动再平衡
有一个重要问题关于再平衡，到底应该自动还是手动？
从全自动到全手动是一个范围光谱。比如，一些数据库执行是自动的，但再平衡之前需要管理员确认。

全自动再平衡是方便，然后不可预测。再平衡是一个昂贵的操作，因为需要路由重定向，节点之间移动大量数据。如果不仔细进行，这个过程会对影响当前负载。
当结合自动失败检测会更危险，造成级联失败。
所以，最好还是有人参与到再平衡的流程中，虽然慢一点，但能避免意外发生。

# Request Routing
我们现在把数据分配到多个节点上了。但一个问题还在：当用户发送一个请求，如何能知道连接哪个节点？分片再平衡，节点变更。谁来回答这个问题，如果要查询key为foo,应该连接哪个IP和端口？

这是一个通用问题被成为服务发现，不仅仅是数据库有关。
从抽象来看，有一些方案：
1. 所有客户端连接任何节点，如果节点有数据，直接处理请求。否则，把请求代理到其他适合节点。
2. 所有请求先发送给路由层，路由层代理请求给其他节点，自己仅仅作为一个partition-aware负载均衡
3. 客户端自己知道分片路由信息，可以直连到节点。

所有以上情况，核心问题都是：需要做出路由决定的组件如何感知分片对应节点（mapping）信息？
这是一个有挑战的问题，因为重点在于所有参与者需要共识，不然请求会被发送到错误节点。在分布式系统中共识很难实现。

大多数分布式数据库依赖zoopkeeper这样的协调服务。节点在zookeeper上注册自己，zookeeper维护一份权威的mapping信息，其他服务订阅它。当分片变更或节点添加删除时，zookeeper通知其他人更新信息。
MongoDB依赖于自己的config server来实现。
Cassandra采用gossip protocol协议，不依赖外部组件。
Counchbase不支持动态再平衡，简化设计。有自己的路由层。

当使用路由层或发送随机节点，客户端还是需要找到具体的IP地址。*这种变更不像分片指定这么频繁，所以采用DNS就足够了*。

## 并行查询执行
我们目前只关注简单的查询scatter/gather，通常NOSql数据都支持。
但有许多分析需求，需要访问大量的数据，这种查询通常都能从并行中获得好处。
MPP 大量并行计算是一个特定领域，后续会深入探讨。

# 总结
我们探寻了不同方式将大数据集分成小的子集。当许多数据在单机上无法排序和处理时分片是必须的。

分片的目标是将数据和负载均匀分配到多台机器上，避免热点。这需要选择合适你数据的分片策略。
- key范围分片，　key被排序，每个分片拥有一个范围key的数据。有效支持范围查询，但相邻的数据都在一起会有热点。
- hash分片，对key进行hash,每个分片拥有一个范围hash值的数据。不支持范围查询，但分布均匀。
  当采用hash分片，通常预先创建固定数量的分片，每个节点指定一定数量的分片，当有节点增加和删除时，在不同节点间移动分片。动态分片通常被使用。

混合方式也可行。比如联合组件，一个字段决定分片，第二个字段决定排序。

通过设计，每个分片的操作基本上是独立的，那样允许一个被分片的数据库分布在多太机器上。然后同时往多个分片写数据就变得困难：比如，如果一个分片成功，一个分片失败怎么办？后面我们会回答这个问题。










