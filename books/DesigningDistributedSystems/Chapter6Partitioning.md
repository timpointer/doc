
# Partitions(sharding)
Partitioning不同系统里面叫法不同，比如shard,region,table,vnode,vBucket，这里统一叫分片。

分片定义了每条数据属于哪个一个分片。有许多不同的分片方式。每个分片就是一个自己的小数据库，虽然数据库可能提供了跨分片的操作。
分片主要目的为了扩展（scalability）。不同的分片分布在不同节点上在一个shared-nothing的集群中。大的数据集合能被放置于不同的磁盘上，请求能被分发到不同的处理器上。

请求能被并行在多个节点上执行。

事务型和分析型数据库在分片上本质是一样的，只是系统调教不同。

我们先看下不同的分片方式和观察数据索引是怎么和分片交互的。然后讨论再平衡，有必要时需要增加和删除节点。最后看下如何路由请求和执行查询。

# Partitioning and Replication
分片和副本是两个独立的概念，可以分开使用也能组合使用。
组合使用的一般场景是，数据集有三个分片，每个分片有三个备份，一主两从。九个分片备份被分布在三个节点上，每个节点上既有主的分片，也有从的分片。

# Partitioning of Key-Value Data
假如你有一个大数据集要分片，如何决定哪条记录在哪个节点上呢？

我们的目标是把数据和请求均匀的分散到不同节点上。理论上１０个节点能处理10倍的数据和10倍的请求。
如果分片不公平，有些分片的数据和请求比其他一些大，称之为"skewed"。当发生偏差时，分片就不够有效了。当请求都集中到一个分片上时被称为"hot spot"

最简单避免热点的方式是随机分片。但这样有一个缺点，就是查询具体一条数据时，需要请求所有节点。
我们能做的更好。假设有一个简单的key-value模型，总是从主键查记录。可以参照百科全书，按照标题排序，你就能快速找到记录。

## 通过key范围分片
一种方法是为一个分片指定一个连续范围的key。如果你知道哪个key属于哪个分片，又知道哪个分片属于哪个节点。就能直接请求具体的节点。

key的范围并不需要平均分布，因为你的数据也不是平均分布的。拿百科全书举例，可能卷1包含A,B开头，卷12包含T,U,V,X,Y和Z。要适应数据本身的特性。
分片可以由管理员手动选择，也能自动选择。

在每个分片中，保持key的排序，好处是方便范围查询。能把key当做组合索引的一部分。
举例，考虑一个感应器时序型应用，拿时间戳作为key来分片，就能很容易查询到一个特定月份的数据。
然而，特定的请求会导致热点。如果按照时间戳分片，一天一个分片，那当天的数据都会写到统一分片中。而其他分片都闲着。

为了避免这个问题，你需要使用时间戳以外的其他元素作为key。比如从感应器的名字加上时间戳来做分片。假设你有许多感应器产生数据，这些请求会被均匀分散。
现在当你需要查询多个感应器的一个时间范围数据时，需要为每隔感应器单独发送请求你。

## HashKey分片
因为偏差和热点的风险，需要分布式数据库采用hash函数来为每个key决定分片。
一个好的hash函数能均分分布，即使两个string很相似，也能在数字中均匀分布。

仅仅为了分片，hash函数不需要强加密，一些数据库使用Fowler-Noll-Vo function。需要语言有自带的hash函数，但它们不都适用于分片，因为java.hashCode()，在不同进程中会得到不同的hash值。

有了合适的hash函数，你可以为每个分片指定一个范围的hash值。这种分片的好处就是key能均匀分布。
分片范围可以均匀分布也可以假均分（有时被称为constistent hashing）。
使用hash避免了需要中央控制或分布式共识。
不幸的时，采用hash分片我们丢失一个重要range分片的特性，支持范围查询。临近的key被分散在所有分片上，所以顺序被丢失了。
比如在mongodb中，如何采用hash分片。任何范围查询都需要发送到所有分片上。

### cassandra的组合分片模式
Cassandra现实了一种权衡，结合了两种策略。一个表能被声明一个联合主键，包含多个字段。只有第一个字段决定分片位置，其他字段为了数据排序用做联合索引。一个请求不能对第一个字段进行范围查询，但指定第一个字段，就能对其他字段进行范围查询。
联合索引很好处理了一对多的模型。比如一个社交网站，用户有很多post，把（user_id, update_timestamp）作为索引，就能高效的查询一个用户的一个时间段的所有post。

## 偏差负载和缓解热点
根据以上讨论，对key进行hash来决定分片能帮助减少热点情况。然而它并不能完全避免，极端情况下所有请求会打到一个key上，最终所有请求都落到一个分片上。

这种请款不常见，但也不罕见。举例，一个社交网站，一些有上百万粉丝的知名大V,当他们做一些事时就可能造成流量风暴。（所有用户都对一个用户进行评论）,hash帮不了，因为同一个key在hash还是一样的值。
当今大多数数据库不能自动解决这个问题。需要用户自己解决，比如可以在key后面增加两个随机数。那样就会有100个不同的key,这些key被分布到不同的分片上。

然后把写分布到不同的key，读的时候就需要额外工作，需要从100key中读取并组合。这需要额外的记事本（bookkeeping），只给少量的热点key添加随机数，其他大量的低请求的key没必要这么做。另外你需要记录哪些key被细分了。
将来可能数据库会解决这个问题，但当下你需要根据情况自己权衡。

# 分片与二级索引
讨论到现在都是基于key-value的数据模型，如果记录都通过主key来访问，我们能通过key决定对应分片，然后把请求路由到负责该记录的分片上。
当情况变复杂时就需要引入二级索引。在关系数据库中二级索引必不可少，也文档数据库里也很常见。许多key-value数据库为避免复杂度而不支持二级索引。而它是搜索引擎的核心。

问题在于二级索引不能简单的对应到分片上，有两种主流方式对二级索引进行分片：document-based分片和term-based分片

## 基于文档分片二级索引
基于文档的索引，也被成为local索引（相对于global）。每个分片中索引只管理自己分片的数据。
写入时，更新数据和更新索引在同一个分片里。但查询时需要采用scatter/gather的方式，请求发送给所有分片，这种操作比较耗时，延迟由最慢的一个分片决定。Mongodb采用这种方式。

## 基于Term分片二级索引
基于Term的索引，可以把索引本身看做是一个数据表，因为要分散负载，所以对索引进行分片，分片的方式和普通数据表一样，可以是范围的也可以是hash。
因为每个key的数据被放在了一起，所以查询时，只需要到特定的分片去查询即可。
缺点是写入时，因为更新数据和索引不在同一个分片上，会既慢有复杂。
理想世界中，索引应该被立即更新。而然这需要分布式事务的支持，不是所有数据库都支持。
实践中，一般都是通过异步来更新global二级索引。DynamoDB保证在秒级别同步二级索引，但也有可能更长。

# 分片再平衡
随着时间流失，一些改变在数据库中发生。
- 请求量增长，需要更多CPU来支持
- 数据集变大，需要更多磁盘和内存来存储
- 机器宕机，需要其他机器接替宕机机器的职责

所有这些改变都需要把数据和请求从一个节点移动到另一个节点。这个过程叫再平衡。
无论哪种分片方式被使用，再平衡都需要满足最小的需求：
- 再平衡以后，负载（数据，读，写请求）被均分分布在集群中
- 当再平衡发生时，服务继续可用
- 只移动必须移动的数据，最小化网络和磁盘IO

## 再平衡策略
有些不同的方式指定分片到节点。

### How not to do it: hash mod N
我们可以mod来决定数据分片，一个分片对应一个节点。假如根据10来取mod。就会有0~9个分片。当增加一个分片，就需要根据11来取mod,这样原来一个key的分片就变了，数据需要移动到其他节点上。再增加一个分片，数据又要被移动。
我们应该想一种方式来避免数据移动。

### 固定数量分片
幸运的是，有一种简单方法。创建比节点多的分片，给每个节点分配多个分片。比如一个集群10个节点，1000个分片均匀分布。
如果新加一个节点，只需要从原来节点上偷走一些分片给到它就行，删除节点，反向操作即可。
仅仅是整个分片被移动，分片数量不变，key和分片对应关系也不变。这种变更不是立即发生的，需要花时间同步大量数据，此时老分片还能继续提供服务。
理论上，你可以给节点分片不同权重的负载（分片数量）。
这种配置方式，需要数据库提前配决定分片数量，之后无法变更。虽然理论上可以切分分片和聚合分片，但大多数数据库拒绝这么做。
分片数量至少要大于节点数量。
如果数据集高度变化，要决定合适的分片数量就很难。最好是"just right"。太大再平衡和节点恢复会很昂贵，太小又有管理成本。

### 动态分片

### Partitioning proportionally to nodes







